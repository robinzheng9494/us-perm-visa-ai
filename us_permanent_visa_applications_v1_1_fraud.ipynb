{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "195b1bd3-262e-4e27-8349-321a36f66482",
        "_uuid": "d589352eba1c01f78b15ada4eb9d53e6f2ff7c41",
        "id": "wpUDJTePSk-X"
      },
      "source": [
        "# General information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "33cf7bed-3db2-4663-8b90-5c0525669fa8",
        "_uuid": "1892062c143e6d1e2198e9d3d7e865b666eeb5a1",
        "id": "YzFSRnm5Sk-Z"
      },
      "source": [
        "The following Jupyter notebook was created in order to derive some meaningful insights from the US Permanent Visa Application decisions. Data covers years 2012 - 2017 and includes information on employer, position, wage offered, job posting history, employee education and past visa history, associated lawyers, and final decision. It was collected and distributed by the US Department of Labor.\n",
        "\n",
        "According to Kaggle's dataset context, a permanent labor certification issued by the Department of Labor (DOL) allows an employer to hire a foreign worker to work permanently in the United States. In most instances, before the U.S. employer can submit an immigration petition to the Department of Homeland Security's U.S. Citizenship and Immigration Services (USCIS), the employer must obtain a certified labor certification application from the DOL's Employment and Training Administration (ETA). The DOL must certify to the USCIS that there are not sufficient U.S. workers able, willing, qualified and available to accept the job opportunity in the area of intended employment and that employment of the foreign worker will not adversely affect the wages and working conditions of similarly employed U.S. workers.\n",
        "\n",
        "The goal of the below data analysis is checking the general trend in Visa applications, the most popular citizenships, employers, cities and finally, predicting the application decision based on the chosen features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "380abe99-d7c9-4a6b-8a19-9ec72bf24f19",
        "_uuid": "7912d638e59acde17434efd7a307a50e84acccc5",
        "id": "MILs98Q8Sk-a"
      },
      "source": [
        "# Importing necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b3179e5d-5b3a-4c26-b520-17e95cb6a42a",
        "_uuid": "26f82f8ba66274a435d1a51f587959e460a103bf",
        "id": "AQBO6MkjSk-a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "f42e310e-e1ad-4ee4-e9ba-5b636e1fad52"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-059de42a76e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Input data files are available in the directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Input data files are available in the directory.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d0cd06eb-567c-4f65-97fc-1bdde9a9f228",
        "_uuid": "753913fdc98704aef0e21cffa965e730ce98433b",
        "id": "JOl2cq37Sk-b"
      },
      "source": [
        "# Exploration of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0e535e65-d8af-4d3e-91f8-9123477c3274",
        "_uuid": "fb578c1654c7786719947cb5e032cfec8063b427",
        "id": "8mG2aGuASk-b"
      },
      "source": [
        "Reading the raw data from the \"us_perm_visas.csv\" file available within Kaggle's datasets into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8d4fdcfc-bc05-4a04-b551-1b7726062ded",
        "_uuid": "fda8bab1237ad1c28408dcde671737bb33ac0454",
        "collapsed": true,
        "id": "falce15DSk-b"
      },
      "outputs": [],
      "source": [
        "# Decision_date and case_recieved_date are read as dates\n",
        "df = pd.read_csv('/content/drive/My Drive/LLM_Data/us_perm_visas.csv', low_memory = False, parse_dates=['decision_date', 'case_received_date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ab4c7171-fa0f-40c1-bc95-f8e2c0058284",
        "_uuid": "ff9b1b955dd23454b6af484a49f50180f69a6ca7",
        "id": "HfYa9_dISk-c"
      },
      "source": [
        "Let's take a look at the structure of our dataset by checking the number of observations, columns and displaying 10 first and last rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aee8e53e-2b04-4db2-9c70-fc256a8dc9f6",
        "_uuid": "5b469249a173b7a2f102b3bf450071a32074b2ae",
        "id": "NF5_RGBmSk-c"
      },
      "outputs": [],
      "source": [
        "# Displaying number of rows and columns\n",
        "print('Number of Visa Applications:', len(df))\n",
        "print('Number of Columns:', len(df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#assume the original cases are not fraudulent for base dataset\n",
        "#append GAN data to original dataframe, add column for fraud\n",
        "#is_fraud\n",
        "\n",
        "#pass_fail -- if fraudulent fail immediately\n",
        "#approved - don't make recommendation\n",
        "#declined - make recommendation\n",
        "#funnelling data we're getting from end users\n",
        "#i.e., approved throw it out for recommendations\n",
        "#declined, keep funnelling and then make recommendation\n",
        "#ML pipeline\n",
        "#keep it as modular as possible\n",
        "#draw the line between all the data points\n",
        "#underfit - not very performative on the training data compared to the test data\n",
        "#two different models - leave the knn model - another model fraudulent/not fraudulent"
      ],
      "metadata": {
        "id": "2uqG9eRDiDVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4a2c1b31-0181-497e-b952-c584a50e5eb6",
        "_uuid": "fef727ae62e2979559d88e60bcc6b4874d152653",
        "id": "lop26ovbSk-c"
      },
      "outputs": [],
      "source": [
        "# Displaying 5 first rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9ad23939-df68-48fe-ab2f-4fbb595a913e",
        "_uuid": "79b354f6aaefa62d605b026a6bb8d72e8de5f275",
        "id": "VnraGFmTSk-c"
      },
      "outputs": [],
      "source": [
        "# Displaying 5 last rows\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ec786a34-f98b-4de9-821f-3691f143cdc9",
        "_uuid": "d69d680be501112f61c63927d17eebac0d406c06",
        "id": "FshbgBchSk-c"
      },
      "source": [
        "As we can see, our data set consists of 374362 applications described by 153 attributes. Let's display their names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4fef96e1-e55d-40f3-90a9-88e588578ff2",
        "_uuid": "158332d2b01ff76a613e74f0636e2f43ea1f865a",
        "id": "P4dMhJAvSk-c"
      },
      "outputs": [],
      "source": [
        "print(df.columns.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5d05ade6-7068-4384-8dd8-acf704389bb9",
        "_uuid": "018261f704f191bf4c08817f29e0b4b25e0d56cd",
        "id": "C0OPJRu8Sk-d"
      },
      "source": [
        "Since we have 2 attributes that may contain similar information - case_number & case_no - let's check their lenghts, number of missing values and example values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2710e144-4b55-407e-9fe1-68374089153c",
        "_kg_hide-input": true,
        "_uuid": "599f8d6c9f12a4a5b319dc2bf40d41d1da68fb9c",
        "id": "ClAPD0lLSk-d"
      },
      "outputs": [],
      "source": [
        "print(\"Length of 'case_number' column is: \", len(df['case_number']),\" with\",df.case_number.isnull().sum(), \"missing values\")\n",
        "print(\"Length of 'case_no' column is: \", len(df['case_no']),\" with\", df.case_no.isnull().sum(),\"missing values \\n\")\n",
        "\n",
        "print(\"First 2 values of case_number column are : \\n\", df['case_number'].head(2),\"\\n\")\n",
        "\n",
        "print(\"Last 2 values of case_number column are : \\n\", df['case_number'].tail(2), \"\\n\")\n",
        "print(\"First 2 values of case_no column are : \\n\", df['case_no'].head(2), \"\\n\")\n",
        "print(\"Last 2 values of case_no column are : \\n\", df['case_no'].tail(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "39e8f006-8d2e-46b5-ac3a-6299e8221e70",
        "_uuid": "3ef40ec726ebc841a381edb992fd277eb45fd691",
        "id": "z-O1Tia0Sk-d"
      },
      "source": [
        "As we supposed, these columns contain similar values and their \"NaN\" values add up to the total number of observations so let's create new column containing only non missing values from both \"case_number\" and \"case_no\" columns and then we will remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "93670c30-bca6-4c82-8c37-26bde952cbca",
        "_uuid": "9f068fa866f8a02cbfe31630c7d6b8c7fd118cf0",
        "collapsed": true,
        "id": "RP1SkV_uSk-d"
      },
      "outputs": [],
      "source": [
        "casenoindex = df.columns.get_loc(\"case_no\")\n",
        "casenumberindex = df.columns.get_loc(\"case_number\")\n",
        "casenumberlist = []\n",
        "\n",
        "for value in df.iloc[0:135269,casenoindex]:\n",
        "    casenumberlist.append(value)\n",
        "\n",
        "for value in df.iloc[135269:374363,casenumberindex]:\n",
        "    casenumberlist.append(value)\n",
        "\n",
        "df['casenumber'] = casenumberlist\n",
        "df.drop(df.columns[[casenoindex,casenumberindex]], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1033dfd1-e28c-4e5c-bb1e-37e1e1788c1a",
        "_uuid": "6836fa52bd391e6ad083e186891d719afae5513d",
        "id": "nsa20-p5Sk-d"
      },
      "source": [
        "Now, let's check the \"case_status\" column as it may contain information about decision made for respective Visa application and print the length of unique values it contains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f6a5796-295d-45b4-be71-225ee4f035ef",
        "_uuid": "22a0f38b3b56cba199dc8963c544084fcd068da0",
        "id": "v8iVUkmtSk-d"
      },
      "outputs": [],
      "source": [
        "#Printing number of unique values for 'case_status' column\n",
        "for value in df.case_status.unique():\n",
        "    print(len(df[df['case_status'] == value]),\" occurrences of status '{}'\".format(value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9695e479-c151-47ad-b6c6-c2bbb180c684",
        "_uuid": "8716e9f661d36cbe5691c773842c7ea846e336a0",
        "id": "iWZ8iUtySk-d"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Since our observations contain some records with status == \"Withdrawn\", we will remove them from our dataset and for cases where status is \"Certified\" or \"Certified-Expired\" we will use just one value \" Certified\" so that we will end up having only the desired values namely \"Certified\" and \"Denied\".  According to Wikipedia and other internet resources, petitioners have 6 months time to file I-140 form after the receiving the status of \"Certified\" before it expires and turns to \"Certified-Expired\" status.\n",
        "\n",
        "Form I-140, Immigrant Petition for Alien Worker is a form submitted to the United States Citizenship and Immigration Services (USCIS) by a prospective employer to petition an alien to work in the US on a permanent basis. This is done in the case when the worker is deemed extraordinary in some sense or when qualified workers do not exist in the US."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "60c15519-06f8-46de-a1fa-5d43ffea7e43",
        "_uuid": "46edbc78e95db8a9b42aab1a9fde80bcce3ddcbb",
        "id": "onjcKNm_Sk-d"
      },
      "outputs": [],
      "source": [
        "#Removing all withdrawn applications\n",
        "df = df[df.case_status != 'Withdrawn']\n",
        "\n",
        "#Combining certified-expired and certified applications and displaying distribution of \"case_status\" variable\n",
        "df.loc[df.case_status == 'Certified-Expired', 'case_status'] = 'Certified'\n",
        "df.case_status.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b25cf27a-1492-44bc-8396-828d074a3f33",
        "_uuid": "5374b64dfc0cf63652a8c936a34fcbaa4f6ab5e5",
        "collapsed": true,
        "id": "-JyCwXPwSk-d"
      },
      "source": [
        "It's interesting that only 7.2% of Visa applications were denied. Now, let's perform dimensionality reduction by removing rows and columns containing only 'NaN' values and check the dataframe's shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "549268fc-9bb6-4973-adec-ffe2bc4be01e",
        "_uuid": "e2c3c7f8c25ff24f340110ff32bfdbac1efb8103",
        "id": "LKIrtyMHSk-d"
      },
      "outputs": [],
      "source": [
        "#Dropping all empty columns\n",
        "df = df.dropna(axis=1, how='all');\n",
        "\n",
        "#Dropping all empty rows\n",
        "df = df.dropna(axis=0, how='all');\n",
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d76c2cc5-23b7-4ab2-83b7-8b257de524c0",
        "_uuid": "ab29e34e56bf957fe2a56c45fc2c2341a94f3624",
        "id": "V2VXwEFnSk-e"
      },
      "source": [
        "It looks like there are neither rows nor columns containing only 'NaN' values so let's check how many columns contains any missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "15aeb80b-53ff-4ef2-a814-cf08c3d837a6",
        "_uuid": "87451615d830dea713e11451cde22b01b4ad86b1",
        "scrolled": false,
        "id": "9BAVt5NASk-e"
      },
      "outputs": [],
      "source": [
        "# Displaying number of missing values in each column\n",
        "for column in df.columns:\n",
        "    print(\"Attribute '{}' contains \".format(column),  df[column].isnull().sum().sum(), \" missing values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d0e07649-f33d-42f0-b5cb-3293c421dd5b",
        "_uuid": "454242a21a40d30afd2eabc6d0b2485a5b082075",
        "id": "ELOtcYo4Sk-e"
      },
      "source": [
        "## Visualization of the unprocessed data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7f589bd0-5a1b-4410-9a89-d7fd1bb7b334",
        "_uuid": "59a36f7e2c4381a24fb21fbb6599fa8615b4c874",
        "id": "_2yHcZzmSk-e"
      },
      "source": [
        "Before removing columns which consist mostly of missing values, let's create a new column containing only the year of Visa application submission and perform some visualisation in order to derive initial insights ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6e64f8e4-b430-428c-8a85-acd21c6ed0e3",
        "_uuid": "c8056f9bcff5361e223ff27bac72ce2b09f7fb9e",
        "id": "HZy-Hs9QSk-e"
      },
      "outputs": [],
      "source": [
        "#Converting the date to contain just the year of application submission\n",
        "df['year'] = df['decision_date'].dt.year\n",
        "\n",
        "#Setting plot parameters\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(12.7, 8.27)\n",
        "sns.set_context(rc={\"font.size\":12})\n",
        "sns.countplot(x=\"year\", hue=\"case_status\", data=df)\n",
        "ax.set(xlabel='Visa application year', ylabel='Number of Visa applicatons')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "83656de7-2ef5-4090-95a9-19fb53e34769",
        "_uuid": "37d09690fb514948a07d8772c2f00e9f7c92ddb1",
        "id": "bFs2szdRSk-e"
      },
      "source": [
        "As we can observe, the number of submitted Visa applications increases every year. It's interesting that while the number of possitively considered applications increases, the number of \"Denied\" ones seems to be similar from year 2013. As a next step, let's see, what where the most popular cities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "101fdc7a-8d39-44ab-8ed4-8f3834e81e3d",
        "_uuid": "73ea8f897346e0ddcd8eef8227e66a01871276cb",
        "id": "W8gRiftiSk-e"
      },
      "outputs": [],
      "source": [
        "# Displaying 15 most popular cities\n",
        "df['employer_city'] = df['employer_city'].str.upper()\n",
        "df['employer_city'].value_counts().head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f2ba9f7d-3740-47a7-a839-0d6ad42861a9",
        "_uuid": "341125b7b0025625451794cca0c3d7d04ad75a70",
        "id": "3_o7Xs2fSk-e"
      },
      "outputs": [],
      "source": [
        "# Setting plot parameters\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(13.7, 8.27)\n",
        "sns.set_context(\"paper\", rc={\"font.size\":12,\"axes.titlesize\":12,\"axes.labelsize\":12})\n",
        "sns.countplot(x='employer_city', hue='year', data=df, order=df.employer_city.value_counts().iloc[:10].index)\n",
        "plt.xticks(rotation=90)\n",
        "ax.set(xlabel='Employer city', ylabel='Number of Visa applications')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2795a637-a888-4b13-a8ff-caf702a6f617",
        "_uuid": "616e7e47f645c943ed0a23c75648d0109af5362c",
        "id": "wP9MZF_0Sk-e"
      },
      "source": [
        "In the last few years, the most popular destination cities were: New York, College Station, Santa Clara, San Jose, Redmond, Mountain View, Houston, SunnyVale, San Francisco and Plano. In most of the cities there was a positive trend in Visa applications. A bizarre situation occured in College Station in 2015 where the number of submitted Visa applications was more or less twice large as in other cities.  \n",
        "\n",
        "\n",
        "Now, let's take a look what were the most hiring employers and economic sectors through these years. For \"us_economic_sector\" variable we have only 120 868 non-missing values, but this should give us an insight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1ea03952-74cd-4988-a6fc-9ac4be88dfb4",
        "_uuid": "37638aa19da4710b57b84f00099fbba73dd79c90",
        "id": "CYftxIGDSk-e"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Setting plot parameters\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(12.7, 8.27)\n",
        "sns.set_context(rc={\"font.size\":12,\"axes.labelsize\":13})\n",
        "sns.countplot(x='employer_name', data=df, palette = sns.cubehelix_palette(8, start=.5, rot=-.75), order=df.employer_name.value_counts().iloc[:10].index)\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Get value counts\n",
        "value_counts = df.job_info_job_title.value_counts().iloc[:10]\n",
        "\n",
        "#Iterating over elements in \"job_info_job_title\" column and displaying counts above bars\n",
        "for i, (category, count) in enumerate(value_counts.items()):\n",
        "        ax.text(i, count, str(count),\n",
        "            horizontalalignment='center',\n",
        "            verticalalignment='bottom')\n",
        "\n",
        "ax.set(xlabel='Employer name', ylabel='Number of Visa applications')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e645a397-db96-4ff8-a275-c21f7b363891",
        "_uuid": "2d18e60d17e0c7f125df39aa17be65b6908f4b15",
        "id": "OWOorDEHSk-e"
      },
      "source": [
        "As we can see, 9 out of 10 most beneficial companies for Visa applicants are IT industry representatives. This leads to the assumption that IT sector is both most favourable and demanding one in United States. Let's check what is the distribution of industries across all Visa applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3a69a239-d6dd-4c77-91e2-e43dbd3b86b9",
        "_uuid": "be9871415fd28aabda6e4704107731039426e010",
        "collapsed": true,
        "id": "awOnpcQSSk-e"
      },
      "outputs": [],
      "source": [
        "#Creating empty dictionary\n",
        "us_economic_counts = {}\n",
        "\n",
        "#Iterating over \"us_economic_sector\" column and appending values to the \"us_economic_counts\" dictionary\n",
        "for value in df['us_economic_sector'].dropna():\n",
        "    if value in us_economic_counts:\n",
        "        us_economic_counts[value] += 1\n",
        "    else:\n",
        "        us_economic_counts[value] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3489ee0a-0932-49bc-bee1-1ba675258282",
        "_uuid": "e310ac627d4b7da107f68bc45ddc60e81a89b786",
        "id": "sx7ha-uYSk-e"
      },
      "outputs": [],
      "source": [
        "#Creating lists for us economic sectors and their counts\n",
        "usecolabels = []\n",
        "usecovalues = []\n",
        "explode = (0.035, 0, 0, 0,0,0,0,0,0,0)\n",
        "\n",
        "for key, value in us_economic_counts.items():\n",
        "    usecolabels.append(key)\n",
        "    usecovalues.append(value)\n",
        "\n",
        "#Setting plot parameters\n",
        "plt.figure(figsize=(13,13))\n",
        "sns.set_context(rc={\"font.size\":10,\"axes.labelsize\":11,\"xtick.labelsize\" : 11})\n",
        "plt.pie(usecovalues[:10], labels=usecolabels[:10], explode = explode, autopct='%1.1f%%', pctdistance = 0.9,\n",
        "          rotatelabels = 90, startangle=140, labeldistance = 1.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c01af4ac-2539-41df-9475-7403108a3bbc",
        "_uuid": "69765c4d5f0b02f75dd96df8cd981143ba743f6e",
        "id": "gIEFQZFaSk-e"
      },
      "source": [
        "Even our US economic sector sample contained only 120 868 non-missing values, this somehow confirms that IT and Advanced Manufacturing are the most convenient sectors for applying foreigners. As a next step in our EDA, let's take a look at the most desired job titles, citizenships and class of admission of our Visa applicants.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "62418e40-b2f2-427c-b897-47e9aff516b1",
        "_uuid": "24ec722143c98c4443b5fac734e8e17fe1ecddef",
        "id": "erfbR-tjSk-e"
      },
      "outputs": [],
      "source": [
        "df['job_info_job_title'].value_counts()[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cc489c48-8d49-4916-90e6-1a1650dd24e7",
        "_uuid": "3f3a42921e5f5a9d2583e1995ea137cb8064b80a",
        "id": "Yz9tQ53HSk-f"
      },
      "source": [
        "Since our column contains job titles with different letter casing we need to standarize them so that\n",
        "value_counts() method will be able to count them more appropriately. Also, there are lots of same positions\n",
        "like \"Computer Systems Analyst\" which differ only by the number standing after hyphen so we will\n",
        "split these titles by finding the '-', 'ii' and '/' signs and leaving only the left side of the splitting\n",
        "result. Afterwards, we are going to remove leading and ending spaces, replace \"sr.\" with \"senior\" values and get rid of  'nan's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "19482d0e-edb5-4d64-b2d5-337172b04e78",
        "_uuid": "8d0c881342378da4fb917726c387afd356add6d6",
        "id": "2rbZ4OreSk-h"
      },
      "outputs": [],
      "source": [
        "#Converting values to lower case\n",
        "df['job_info_job_title'] = df['job_info_job_title'].str.lower()\n",
        "\n",
        "#Splitting job titles by '-'\n",
        "df['job_info_job_title'] = df['job_info_job_title'].astype(str).str.split('-').str[0]\n",
        "#Splitting job titles by 'ii'\n",
        "df['job_info_job_title'] = df['job_info_job_title'].astype(str).str.split('ii').str[0]\n",
        "#Splitting job titles by '/'\n",
        "df['job_info_job_title'] = df['job_info_job_title'].astype(str).str.split('/').str[0]\n",
        "#Removing leading and ending spaces\n",
        "df['job_info_job_title'] = df['job_info_job_title'].astype(str).str.strip()\n",
        "#Replacing \"sr.\" values with \"senior\"\n",
        "df['job_info_job_title'] = df['job_info_job_title'].str.replace('sr.', 'senior')\n",
        "#Replacing \"NaN\", \"NaT\" and \"nan\" values with np.nan\n",
        "df['job_info_job_title'].replace([\"NaN\", 'NaT','nan'], np.nan, inplace = True)\n",
        "\n",
        "\n",
        "df['job_info_job_title'].value_counts(dropna=True)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bb8fd874-0bcd-4954-9240-06c2660ff6fd",
        "_uuid": "c7cfc9aa9f8066ab3e8190e8d18e43ffce9feae8",
        "id": "qgKgkmE9Sk-i"
      },
      "outputs": [],
      "source": [
        "#Setting plot parameters\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(12.7, 8.27)\n",
        " #sns.set_context(rc={\"font.size\":14, \"axes.labelsize\":12})\n",
        "sns.countplot(x='job_info_job_title',data=df,\n",
        "               palette = sns.diverging_palette(255, 133, l=60, n=10, center=\"dark\"),\n",
        "               order=df.job_info_job_title.value_counts().iloc[:10].index)\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Get value counts\n",
        "value_counts = df.job_info_job_title.value_counts().iloc[:10]\n",
        "\n",
        "#Iterating over elements in \"job_info_job_title\" column and displaying counts above bars\n",
        "for i, (category, count) in enumerate(value_counts.items()):\n",
        "        ax.text(i, count, str(count),\n",
        "            horizontalalignment='center',\n",
        "            verticalalignment='bottom')\n",
        "\n",
        "#Setting label titles\n",
        "ax.set(xlabel='Job Title', ylabel='Number of Visa applications')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "544e0ee7-a407-49fd-9819-30f4d413eda8",
        "_uuid": "bc4efbc48fae2b3f6e7eb1b834670e73e3b72fd7",
        "id": "lbHGG6oWSk-i"
      },
      "source": [
        "Interestingely, all of the most popular positions except  \"assistant professor\" are derived from the IT industry. This is another confirmation that there is a huge demand for IT specialists in USA and being one of them increases our chances to obtain a permanent Visa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "005b01cd-ba72-49cf-bc67-27819abc3ae7",
        "_uuid": "8eb8c22e146ca38e6dfdbb2308a48656d33a99bc",
        "id": "w2n73Ku3Sk-i"
      },
      "outputs": [],
      "source": [
        "#Setting plot parameters\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(12.7, 8.27)\n",
        "sns.set_context(rc={\"font.size\":14, \"axes.labelsize\":12})\n",
        "sns.countplot(x='country_of_citizenship',hue='case_status',data=df,\n",
        "              palette = sns.diverging_palette(255, 133, l=60, n=7, center=\"dark\"),\n",
        "              order=df.country_of_citizenship.value_counts().iloc[:7].index)\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Get value counts\n",
        "value_counts = df.country_of_citizenship.value_counts().iloc[:7]\n",
        "\n",
        "#Iterating over elements in \"country_of_citizenship\" column and displaying counts above bars\n",
        "for i, (category, count) in enumerate(value_counts.items()):\n",
        "        ax.text(i, count, str(count),\n",
        "            horizontalalignment='center',\n",
        "            verticalalignment='bottom')\n",
        "\n",
        "#Setting label titles\n",
        "ax.set(xlabel='Country of citizenship', ylabel='Number of Visa applications')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "55baf0ec-4da0-454c-8040-7c9c9864bf98",
        "_uuid": "08a5ade699adb7df5402be36969df7df1ec349b6",
        "id": "XIq5LZn5Sk-i"
      },
      "source": [
        "As we can see, the majority of Visa applications has been submitted by Indian citizens. They constitute to more than half of our observations, we can assume that most of them are computer specialists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "395c73ab-caf2-4b62-bab5-e91402ea7437",
        "_uuid": "5cfcd2e02edfdc278612b023a00fd26a8208d5bb",
        "id": "sYwNIVlGSk-i"
      },
      "outputs": [],
      "source": [
        "# Setting plot parameters\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(12.7, 8.27)\n",
        "\n",
        "# Create countplot\n",
        "sns.countplot(x='class_of_admission', data=df,\n",
        "              order=df.class_of_admission.value_counts().iloc[:10].index)\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Get value counts\n",
        "value_counts = df.class_of_admission.value_counts().iloc[:10]\n",
        "\n",
        "# Iterating over elements and displaying counts above bars\n",
        "for i, (category, count) in enumerate(value_counts.items()):\n",
        "    ax.text(i, count, str(count),\n",
        "            horizontalalignment='center',\n",
        "            verticalalignment='bottom')\n",
        "\n",
        "ax.set(xlabel='Visa type', ylabel='Number of Visa applications')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c306571b-7e95-49f9-9a7b-e6eb7ddc6c79",
        "_uuid": "2548a866584ca0fbd83ee096c7b50602c43967a1",
        "id": "8nc__wMaSk-i"
      },
      "source": [
        "The vast majority of petitioners were applying for the H-1B Visa, which according to the Wikipedia, allows U.S. employers to employ foreign workers in specialty occupations. If a foreign worker in H-1B status quits or is dismissed from the sponsoring employer, the worker must either apply for and be granted a change of status, find another employer (subject to application for adjustment of status and/or change of visa), or leave the United States.\n",
        "\n",
        "\n",
        "Finally, let's try checking on the number and kind of application types. Unfortunately, our data consists only of 126 848 non-missing values for this attribute, but this should give us a general overview."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1887dd8f-2631-4c3f-bf58-a61a6c11c4e0",
        "_uuid": "af267987a55efc1bde176f7ea34ce0a884704ec3",
        "id": "q0_6wns7Sk-i"
      },
      "outputs": [],
      "source": [
        "#Setting plot parameters\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(10.7, 7.27)\n",
        "sns.countplot(x='application_type', data=df, palette = sns.color_palette(\"GnBu_d\"), order=df.application_type.value_counts().iloc[:10].index)\n",
        "\n",
        "# Get value counts\n",
        "value_counts = df.application_type.value_counts().iloc[:10]\n",
        "\n",
        "# Iterating over elements and displaying counts above bars\n",
        "for i, (category, count) in enumerate(value_counts.items()):\n",
        "        ax.text(i, count, str(count),\n",
        "            horizontalalignment='center',\n",
        "            verticalalignment='bottom')\n",
        "\n",
        "ax.set(xlabel='Application type', ylabel='Number of Visa applications')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "94e5a222-8d4f-4af7-a98c-7ff982ea358a",
        "_uuid": "deb3611ac6d57fc4b9885aaa25138d2b79ea16cd",
        "id": "I80nBXoiSk-i"
      },
      "source": [
        "Online submission was the most popular form of application type. Here, we can also find \"PERM\" value which is probably incorrect. My assumtion is that some petitioners thought about this form field as a distinction between \"temporary\" and \"permanent\" Visa type. The last plotting activity will be displaying the applicants education level and remuneration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "45bd1489-4295-48b5-b372-63176eebe6d5",
        "_uuid": "00786db794fe9b55d43102f20b60a172ec027be4",
        "id": "nrZhPn-PSk-i"
      },
      "outputs": [],
      "source": [
        "#Setting plot parameters\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(12.7, 8.27)\n",
        " #sns.set_context(rc={\"font.size\":14, \"axes.labelsize\":12})\n",
        "sns.countplot(x='foreign_worker_info_education',data=df,\n",
        "               palette = sns.color_palette(\"Paired\"),\n",
        "               order=df.foreign_worker_info_education.value_counts().iloc[:10].index)\n",
        "\n",
        "# Get value counts\n",
        "value_counts = df.foreign_worker_info_education.value_counts().iloc[:10]\n",
        "\n",
        "# Iterating over elements and displaying counts above bars\n",
        "for i, (category, count) in enumerate(value_counts.items()):\n",
        "        ax.text(i, count, str(count),\n",
        "            horizontalalignment='center',\n",
        "            verticalalignment='bottom')\n",
        "\n",
        "#Setting label titles\n",
        "ax.set(xlabel='Education level', ylabel='Number of Visa applications')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6484afd9-3fe3-495f-8ff7-aaf1c78dc485",
        "_uuid": "e1d876f709a63c4a5b98e98776cc3d31c5df2c3f",
        "id": "u5HN_BXBSk-i"
      },
      "source": [
        "As we can see, over 50% of applicants obtained a university degree. Before plotting the remuneration, we will remove commas from the values so that they are left only with decimal places denoted. Also, since some of the wages are hourly, weekly, bi-weekly and monthly values, we have to calculate the yearly equivalents for them. According to the https://www.timeanddate.com/date/workdays.html website, the average number of working days in USA is 250. We will use this information in our calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6d449bab-9a89-40f9-b854-0afb062b9b3d",
        "_uuid": "589bc80b820532d3b9fd6ca567611940c4e81a81",
        "id": "WjToHvamSk-i"
      },
      "outputs": [],
      "source": [
        "df[['pw_amount_9089','pw_unit_of_pay_9089']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8cadf005-466b-4d1d-bb11-0388bd0c92e0",
        "_uuid": "4fa67723698fce63cbf8c874bdf6e2b8f5ea0ffd",
        "id": "O5YU2fcCSk-i"
      },
      "outputs": [],
      "source": [
        "# First clean the data\n",
        "df['pw_amount_9089'] = df['pw_amount_9089'].str.replace(\",\", \"\").str.strip()\n",
        "\n",
        "# Function to safely convert to float\n",
        "def safe_float_convert(x):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "# Process the different pay units\n",
        "for unit in df.pw_unit_of_pay_9089.unique():\n",
        "    if unit == \"hr\" or unit == \"Hour\":\n",
        "        df.loc[df['pw_unit_of_pay_9089'] == unit, 'pw_amount_9089'] = df.loc[df['pw_unit_of_pay_9089'] == unit, 'pw_amount_9089'].apply(safe_float_convert) * 8 * 250\n",
        "        df.loc[df['pw_unit_of_pay_9089'] == unit, 'pw_unit_of_pay_9089'] = \"Year\"\n",
        "    elif unit == \"wk\" or unit == \"Week\":\n",
        "        df.loc[df['pw_unit_of_pay_9089'] == unit, 'pw_amount_9089'] = df.loc[df['pw_unit_of_pay_9089'] == unit, 'pw_amount_9089'].apply(safe_float_convert) * 50\n",
        "        df.loc[df['pw_unit_of_pay_9089'] == unit, 'pw_unit_of_pay_9089'] = \"Year\"\n",
        "    elif unit == \"mth\" or unit == \"Month\":\n",
        "        df.loc[df['pw_unit_of_pay_9089'] == unit, 'pw_amount_9089'] = df.loc[df['pw_unit_of_pay_9089'] == unit, 'pw_amount_9089'].apply(safe_float_convert) * 12\n",
        "        df.loc[df['pw_unit_of_pay_9089'] == unit, 'pw_unit_of_pay_9089'] = \"Year\"\n",
        "    elif unit == \"bi\" or unit == \"Bi-Weekly\":\n",
        "        df.loc[df['pw_unit_of_pay_9089'] == unit, 'pw_amount_9089'] = df.loc[df['pw_unit_of_pay_9089'] == unit, 'pw_amount_9089'].apply(safe_float_convert) * 25\n",
        "        df.loc[df['pw_unit_of_pay_9089'] == unit, 'pw_unit_of_pay_9089'] = \"Year\"\n",
        "    elif unit == \"yr\":\n",
        "        df.loc[df['pw_unit_of_pay_9089'] == unit, 'pw_unit_of_pay_9089'] = \"Year\"\n",
        "\n",
        "# Fill missing values with median after converting all to float\n",
        "df['pw_amount_9089'] = pd.to_numeric(df['pw_amount_9089'], errors='coerce')\n",
        "df['pw_amount_9089'] = df['pw_amount_9089'].fillna(df['pw_amount_9089'].median())\n",
        "\n",
        "# Display results\n",
        "print(df[['pw_amount_9089', 'pw_unit_of_pay_9089']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cbf4e996-1f51-4425-b6df-4b9de6f770a2",
        "_uuid": "0b2f9923723647e67ce16310b30316ec8edc5bb2",
        "id": "w5bHwKAlSk-j"
      },
      "outputs": [],
      "source": [
        "#Since running \"describe\" method on \"pw_amount_9089\" column returned exponential values, I decided to\n",
        "#convert them to floats so that they are easier to understand\n",
        "with pd.option_context('float_format', '{:.2f}'.format): print(df.pw_amount_9089.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "66a71e92-b61c-491c-8b4c-1315e94390ba",
        "_uuid": "bcab941d284d2eeb2621ff3954d24778168e7506",
        "id": "DqH58UzoSk-j"
      },
      "outputs": [],
      "source": [
        "#Dividing our continuous income values into some categories to facilitate their visualization\n",
        "df['remuneration'] = pd.cut(df['pw_amount_9089'], [0, 30000, 60000,90000,120000,150000,180000,210000,240000,270000,495748000], right=False, labels=[\"0-30k\", \"30-60k\",\"60-90k\",\"90-120k\",\"120-150k\",\"150-180k\",\"180-210k\",\"210-240k\",\"240-270k\",\"270k+\"])\n",
        "salary = df['remuneration'].value_counts()\n",
        "salary.iloc[np.argsort(salary.index)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "93efe5d2-bc16-4e71-95ae-2675a2959f99",
        "_uuid": "a2305607abd610ea226db2d721b4f62322f57ac0",
        "id": "O3eQZ8wfSk-j"
      },
      "outputs": [],
      "source": [
        "# Draw a count plot to show the distribution of remunerations\n",
        "# Using catplot instead of factorplot\n",
        "g = sns.catplot(x='remuneration', data=df, kind=\"count\",\n",
        "                palette=\"BuPu\", height=9, aspect=1.2)\n",
        "g.set(xlabel='Remuneration', ylabel='Number of applicants')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "12429beb-b034-4f96-b52c-d04a1038221f",
        "_uuid": "1e67321477054465050ca3c82f50af7abc94aa1f",
        "id": "E_cCl-blSk-j"
      },
      "source": [
        "As we can see, over 65% of the applicants earn between 60 and 120 thousand dollars yearly.  From this moment, we will start working on the feature selection and data cleansing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "036b8625-221b-4dce-9983-4bbdea7e31f8",
        "_uuid": "d318da0a2d46c45e0cb51a8b9b18183eec7dbb04",
        "id": "SBYRRHgTSk-j"
      },
      "source": [
        "## Feature selection and data cleansing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "edfe9cd5-7bcd-4e83-861f-e2ab6325f51a",
        "_uuid": "907ea14ff1d209a5b8fd18131a9c316d010e6217",
        "id": "kCQFh_vHSk-j"
      },
      "outputs": [],
      "source": [
        "#Displaying percentage of non-null values for each feature\n",
        "i = 0;\n",
        "for col in df.columns:\n",
        "    i = i+1;\n",
        "    print (i-1,\"Column: '{}'\".format(col),\"contains \", np.round(100*df[col].count()/len(df['case_status']),decimals=2),\"% non-null values\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dad63284-ff68-448e-923b-be4d0b25613b",
        "_uuid": "916d73fba9d829fa476d4c5df2156bf83812cff4",
        "id": "ek0I7UrUSk-j"
      },
      "outputs": [],
      "source": [
        "#Leaving columns which have more than 330000 non-missing observations\n",
        "df = df.loc[:,df.count() >= 330000]\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dc201155-a96d-4742-a57c-47a686f14595",
        "_uuid": "01efd899916da67cdc857038267e81b8676f4edf",
        "id": "6QVd8_oESk-j"
      },
      "source": [
        "Since our dataset consists of 19 attributes which have less than 12% of missing values , we will choose some of them for further analysis and perform imputations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e6edc1f8-553c-4a06-b3e0-2dad8720efda",
        "_uuid": "105d887a8baf0180274f585ccce173e54a00c57a",
        "collapsed": true,
        "id": "MJ-PL9jjSk-j"
      },
      "outputs": [],
      "source": [
        "#Indices of selected features\n",
        "chosen_attrs = [0,1,2,5,6,8,12,14,17,18]\n",
        "df = df.iloc[:,chosen_attrs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d7b5d579-6507-4110-953c-18539e39f1a2",
        "_uuid": "d34ee5d1960acf930efd6814067dc26c2b14b2f3",
        "collapsed": true,
        "id": "4r-N_0RmSk-j"
      },
      "outputs": [],
      "source": [
        "#Assigning Labels to Case Status\n",
        "df.loc[df.case_status == 'Certified', 'case_status'] = 1\n",
        "df.loc[df.case_status == 'Denied', 'case_status'] = 0\n",
        "\n",
        "#Filling missing values in \"employer_state\" column with mode\n",
        "df['employer_state'] = df['employer_state'].fillna(df['employer_state'].mode()[0]);\n",
        "\n",
        "#Mapping from state name to abbreviation\n",
        "state_abbrevs = {\n",
        "    'Alabama': 'AL',\n",
        "    'Alaska': 'AK',\n",
        "    'Arizona': 'AZ',\n",
        "    'Arkansas': 'AR',\n",
        "    'California': 'CA',\n",
        "    'Colorado': 'CO',\n",
        "    'Connecticut': 'CT',\n",
        "    'Delaware': 'DE',\n",
        "    'Florida': 'FL',\n",
        "    'Georgia': 'GA',\n",
        "    'Hawaii': 'HI',\n",
        "    'Idaho': 'ID',\n",
        "    'Illinois': 'IL',\n",
        "    'Indiana': 'IN',\n",
        "    'Iowa': 'IA',\n",
        "    'Kansas': 'KS',\n",
        "    'Kentucky': 'KY',\n",
        "    'Louisiana': 'LA',\n",
        "    'Maine': 'ME',\n",
        "    'Maryland': 'MD',\n",
        "    'Massachusetts': 'MA',\n",
        "    'Michigan': 'MI',\n",
        "    'Minnesota': 'MN',\n",
        "    'Mississippi': 'MS',\n",
        "    'Missouri': 'MO',\n",
        "    'Montana': 'MT',\n",
        "    'Nebraska': 'NE',\n",
        "    'Nevada': 'NV',\n",
        "    'New Hampshire': 'NH',\n",
        "    'New Jersey': 'NJ',\n",
        "    'New Mexico': 'NM',\n",
        "    'New York': 'NY',\n",
        "    'North Carolina': 'NC',\n",
        "    'North Dakota': 'ND',\n",
        "    'Ohio': 'OH',\n",
        "    'Oklahoma': 'OK',\n",
        "    'Oregon': 'OR',\n",
        "    'Pennsylvania': 'PA',\n",
        "    'Rhode Island': 'RI',\n",
        "    'South Carolina': 'SC',\n",
        "    'South Dakota': 'SD',\n",
        "    'Tennessee': 'TN',\n",
        "    'Texas': 'TX',\n",
        "    'Utah': 'UT',\n",
        "    'Vermont': 'VT',\n",
        "    'Virginia': 'VA',\n",
        "    'Washington': 'WA',\n",
        "    'West Virginia': 'WV',\n",
        "    'Wisconsin': 'WI',\n",
        "    'Wyoming': 'WY',\n",
        "    'Northern Mariana Islands':'MP',\n",
        "    'Palau': 'PW',\n",
        "    'Puerto Rico': 'PR',\n",
        "    'Virgin Islands': 'VI',\n",
        "    'District of Columbia': 'DC'\n",
        "}\n",
        "\n",
        "#Capitalizing Keys\n",
        "us_state_abbrev = {k.upper(): v for k, v in state_abbrevs.items()}\n",
        "df['employer_state'].replace(us_state_abbrev, inplace=True)\n",
        "df.employer_state = df.employer_state.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5ca45544-531d-41bc-828e-8db2c5d7309b",
        "_uuid": "29471b2cfa236a7f5ed5b1aad740c421f4772f0c",
        "collapsed": true,
        "id": "4JIoorzvSk-j"
      },
      "outputs": [],
      "source": [
        "df['pw_soc_code'] = df['pw_soc_code'].str.replace('.','')\n",
        "df['pw_soc_code'] = df['pw_soc_code'].str.replace('-','')\n",
        "df['pw_soc_code'] = df['pw_soc_code'].astype(str).str[0:6]\n",
        "df['pw_soc_code'].value_counts()\n",
        "\n",
        "#Finding \"nan\" values in \"pw_soc_code\" column and filling them with mode\n",
        "df.loc[df['pw_soc_code'] == \"nan\",'pw_soc_code'] = df['pw_soc_code'].mode()[0]\n",
        "\n",
        "#Finding \"None\" values in \"pw_soc_code\" column and filling them with mode\n",
        "df.loc[df['pw_soc_code'] == \"None\",'pw_soc_code'] = df['pw_soc_code'].mode()[0]\n",
        "\n",
        "#Changing type from string to int\n",
        "df['pw_soc_code'] = df['pw_soc_code'].astype(int)\n",
        "df['case_status'] = df['case_status'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "438da8f4-a22d-4717-b841-02d0f9279eae",
        "_uuid": "184c2c7695270b08e93901e734e5daa2c5f0baf7",
        "collapsed": true,
        "id": "wIrceYGqSk-j"
      },
      "outputs": [],
      "source": [
        "#Replacing missing values with mode\n",
        "df['class_of_admission']=df['class_of_admission'].fillna((df['class_of_admission'].mode()[0]))\n",
        "df['country_of_citizenship']=df['country_of_citizenship'].fillna((df['country_of_citizenship'].mode()[0]))\n",
        "df['employer_city']=df['employer_city'].fillna((df['employer_city'].mode()[0]))\n",
        "df['employer_name']=df['employer_name'].fillna((df['employer_name'].mode()[0]))\n",
        "df['employer_name']=df['employer_name'].astype(str).str.upper()\n",
        "df['pw_source_name_9089']=df['pw_source_name_9089'].fillna((df['pw_source_name_9089'].mode()[0]))\n",
        "df['remuneration']=df['remuneration'].fillna((df['remuneration'].mode()[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ba4f5e4f-007e-4c1f-b434-266ac40d0b52",
        "_uuid": "41255e6b18fb2d3a2ac982572f03820decd95562",
        "id": "Y_uS1LeJSk-j"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4d04fd41-d84b-452b-b81f-92cde4a99242",
        "_uuid": "e22f9b97556c4a66874d251f61b1b52df0f62f28",
        "id": "qdYx-7hxSk-j"
      },
      "source": [
        "## Data type conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5e1c8566-6a8c-4c0e-abf0-5ba45026e685",
        "_uuid": "4a3816af535da733df82802c75c532edc33cc34b",
        "id": "vjywODvgSk-k"
      },
      "source": [
        "In this step we're going to turn our feature variables into categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "66202c79-140b-4e1a-8898-b2758b653803",
        "_uuid": "3a4ea1024b5fac7bef5a699f41e7df9b46f44ae2",
        "id": "FHn_-Tj0Sk-k"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "categorical_variables = {}\n",
        "\n",
        "#Creating categories denoted by integers from column values\n",
        "for col in df.columns:\n",
        "    cat_var_name = \"cat_\"+ col\n",
        "    cat_var_name = LabelEncoder()\n",
        "    cat_var_name.fit(df[col])\n",
        "    df[col] = cat_var_name.transform(df[col])\n",
        "    categorical_variables[col] = cat_var_name\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f85cb77b-8472-49fc-bf8c-02879951c785",
        "_uuid": "fe1137556c4b5ce29a68eea11157fdea2cae17ae",
        "id": "_uRp9N8dSk-k"
      },
      "source": [
        "# Applying Machine Learning algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "826112a5-903f-4cc9-bc7d-59cd227e73d8",
        "_uuid": "7d5b5a780bbb89e9ad134eee4d73937d57f92e06",
        "id": "S6-wc_KnSk-k"
      },
      "source": [
        "First thing we're going to do in this part of our analysis will be dividing our final dataset into 2 dataframes. First one will consist of feature variables and the second one only of our target variable - case_status. Afterward we will\n",
        "use GridSearch object with cross-validation to find the best parameters for Logistic Regression, k-Nearest Neighbor, Random Forest and Gradient Boosting Classifiers and evaluate how well they will generalize. Cross validation will split the data repeatedly using Stratified K-Folds cross-validator and train multiple models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f199063d-1578-425b-a282-c5e42d986a19",
        "_uuid": "41b61502a4a5209954cfc5a09b1f9467f49e3c11",
        "id": "80hi3xrcSk-k"
      },
      "outputs": [],
      "source": [
        "#Dividing our final dataset into features(explanatory variables) and labels(target variable)\n",
        "#!!!!\n",
        "#Take initial\n",
        "\n",
        "#df.add_column[\"fraud\"]\n",
        "#df.set all the rows in original data frame to 0 for honest applications\n",
        "#GAN logic generated rows for data frame\n",
        "#GAN_df add_column(fraud )\n",
        "#set all the rows in GAN df to 1\n",
        "#x=df.loc[:, df.columns != \"fraud\"]\n",
        "#y=df.loc[] fraud\n",
        "#loss function, plot\n",
        "#350,000 evenly split data between honest and lie\n",
        "#360,000 rows,\n",
        "X = df.loc[:, df.columns != 'case_status']\n",
        "y = df.case_status\n",
        "\n",
        "print(\"The shape of X is: {}\".format(X.shape))\n",
        "print(\"The shape of y is: {}\".format(y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "69c6cfbe-0021-4d3c-ae0b-fcfc3d38e019",
        "_uuid": "feb7debca43a9ccd1f7ea78ea77251fffc478e0a",
        "id": "U9rTvOe0Sk-k"
      },
      "outputs": [],
      "source": [
        "#Fit Algorithm\n",
        "#Importing Logistic Regression Classifier, GridSearchCV, train_test_split and accuracy metrics from sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "#Defining fit_algorithm function\n",
        "def fit_algorithm(alg, X, y, parameters, cv = 5):\n",
        "    \"\"\"\n",
        "    This function will split our dataset into training and testing subsets, fit cross-validated\n",
        "    GridSearch object, test it on the holdout set and return some statistics\n",
        "    \"\"\"\n",
        "    #instead of certified or denied, train on fraudulent/not fraudulent\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 123)\n",
        "    grid = GridSearchCV(alg, parameters, cv = cv)\n",
        "    grid.fit(X_train, y_train)\n",
        "    y_pred = grid.predict(X_test)\n",
        "    confmat = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "    return pd.Series({\n",
        "        \"Train_ACC\": np.around(grid.best_score_, decimals=2).astype(str),\n",
        "        \"Test_ACC\": np.around(grid.score(X_test, y_test), decimals=2).astype(str),\n",
        "        \"P\": np.around(precision_score(y_pred, y_test), decimals=2).astype(str),\n",
        "        \"R\": np.around(recall_score(y_pred, y_test),decimals=2).astype(str),\n",
        "        \"F1\": np.around(f1_score(y_pred, y_test),decimals=2).astype(str),\n",
        "        \"Best_params\": [grid.best_params_],\n",
        "        \"True negatives\": confmat[0,0],\n",
        "        \"False negatives\": confmat[1,0],\n",
        "        \"True positives\": confmat[1,1],\n",
        "        \"False positives\": confmat[0,1]\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "29524708-9f98-4c67-bab6-cc6e8e1288fc",
        "_uuid": "4dbaf5f5a9953e43d3b99c4f5a8b05f466c27e7d",
        "id": "qRYl4GvqSk-k"
      },
      "source": [
        "## k-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2e3a3afc-0fcd-4678-9a85-439ecdd3dc75",
        "_uuid": "0234a3c72841e1dcc4a3356f55741c6d159d5a9a",
        "id": "YsfDVYIPSk-k"
      },
      "outputs": [],
      "source": [
        "#Importing k-Nearest Neighbors Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#Defining range of parameters for kNN Clssifier\n",
        "knn_params = {'n_neighbors': np.arange(1,11).tolist()}\n",
        "\n",
        "#Using \"fit_algorithm\" function with kNN Classifier\n",
        "knn = fit_algorithm(KNeighborsClassifier(),X,y,knn_params)\n",
        "knn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "db9b07e3-ea57-457a-aa11-b83c847b0b0e",
        "_uuid": "da2e16ca889d2e71a7a7d8c79ea2236332313d65",
        "id": "WkLGELf2Sk-k"
      },
      "source": [
        "Interestingly, the k-Nearest Neighbors Classifier achieved the same accuracy score as the Logistic Regression Classifier. Even accuracy is the same, there are slight differences in Precision, Recall, True Positive, True Negative, False Positive and False Negative values. Now, let's check, how precise the Random Forest Classifier can be."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Layer architecture\n",
        "        self.gen = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, output_dim),\n",
        "            nn.Tanh()  # Output between -1 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.disc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()  # Output probability between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AssQ0AJUZuSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class H1BGAN:\n",
        "    def __init__(self, real_data):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # First preprocess the data\n",
        "        self.original_data = real_data.copy()\n",
        "        self.original_columns = real_data.columns\n",
        "        self.real_data, self.encoders = self._preprocess_data(real_data)\n",
        "\n",
        "        # Set dimensions\n",
        "        self.input_dim = 100\n",
        "        self.output_dim = self.real_data.shape[1]\n",
        "\n",
        "        # Initialize networks\n",
        "        self.generator = Generator(self.input_dim, self.output_dim).to(self.device)\n",
        "        self.discriminator = Discriminator(self.output_dim).to(self.device)\n",
        "\n",
        "        # Initialize optimizers\n",
        "        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        self.d_optimizer = optim.Adam(self.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Store original data and column names\n",
        "        self.original_data = real_data.copy()\n",
        "        self.original_columns = real_data.columns\n",
        "\n",
        "        # Preprocess the data\n",
        "        self.real_data, self.encoders = self._preprocess_data(real_data)\n",
        "\n",
        "        self.input_dim = 100  # Noise dimension\n",
        "        self.output_dim = self.real_data.shape[1]  # Updated dimension after preprocessing\n",
        "\n",
        "    def _preprocess_data(self, data):\n",
        "        \"\"\"Preprocess the data by encoding categorical variables and scaling numerical ones\"\"\"\n",
        "        from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "        import pandas as pd\n",
        "\n",
        "        processed_data = data.copy()\n",
        "        encoders = {}\n",
        "\n",
        "        # Convert dates to timestamps\n",
        "        date_columns = [col for col in processed_data.columns if 'date' in col.lower()]\n",
        "        for col in date_columns:\n",
        "            try:\n",
        "                processed_data[col] = pd.to_datetime(processed_data[col]).astype(np.int64) // 10**9\n",
        "            except:\n",
        "                # If conversion fails, drop the column\n",
        "                processed_data = processed_data.drop(col, axis=1)\n",
        "\n",
        "        # Identify categorical columns (object dtype)\n",
        "        categorical_columns = processed_data.select_dtypes(include=['object']).columns\n",
        "\n",
        "        # Encode categorical variables\n",
        "        for col in categorical_columns:\n",
        "            encoders[col] = LabelEncoder()\n",
        "            processed_data[col] = encoders[col].fit_transform(processed_data[col].astype(str))\n",
        "\n",
        "        # Scale all columns to [0, 1] range\n",
        "        scaler = MinMaxScaler()\n",
        "        processed_data = pd.DataFrame(\n",
        "            scaler.fit_transform(processed_data),\n",
        "            columns=processed_data.columns\n",
        "        )\n",
        "        encoders['scaler'] = scaler\n",
        "\n",
        "        return processed_data, encoders\n",
        "\n",
        "        # Initialize networks\n",
        "        self.generator = Generator(self.input_dim, self.output_dim).to(self.device)\n",
        "        self.discriminator = Discriminator(self.output_dim).to(self.device)\n",
        "\n",
        "        # Optimizers\n",
        "        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        self.d_optimizer = optim.Adam(self.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "    def generate_fraudulent_patterns(self):\n",
        "        \"\"\"Generate specific fraudulent patterns based on actual columns\"\"\"\n",
        "        patterns = {\n",
        "            'wage_mismatch': {\n",
        "                'wage_offer_from_9089': 150000,  # Suspiciously high wage\n",
        "                'job_info_experience_num_months': 12,  # Entry level experience\n",
        "                'job_info_education': 'Bachelor\\'s'\n",
        "            },\n",
        "            'experience_education_mismatch': {\n",
        "                'job_info_education': 'High School',\n",
        "                'job_info_job_title': 'Senior Software Engineer',\n",
        "                'job_info_experience_num_months': 6  # Too little experience\n",
        "            },\n",
        "            'application_timeline_mismatch': {\n",
        "                'case_received_date': 'recent',\n",
        "                'decision_date': 'very_quick'  # Suspicious quick approval\n",
        "            }\n",
        "        }\n",
        "        return patterns\n",
        "\n",
        "    def inject_fraud_patterns(self, generated_data):\n",
        "        \"\"\"Inject fraudulent patterns into generated data\"\"\"\n",
        "        fraud_patterns = self.generate_fraudulent_patterns()\n",
        "\n",
        "        # Randomly select records to inject fraud patterns\n",
        "        fraud_indices = np.random.choice(\n",
        "            len(generated_data),\n",
        "            size=int(len(generated_data) * 0.15),  # 15% fraudulent applications\n",
        "            replace=False\n",
        "        )\n",
        "\n",
        "        for idx in fraud_indices:\n",
        "            pattern = np.random.choice(list(fraud_patterns.keys()))\n",
        "\n",
        "            if pattern == 'wage_mismatch':\n",
        "                if 'wage_offer_from_9089' in generated_data.columns:\n",
        "                    # Double the wage for entry level positions\n",
        "                    generated_data.loc[idx, 'wage_offer_from_9089'] = \\\n",
        "                        generated_data.loc[idx, 'wage_offer_from_9089'] * 2\n",
        "\n",
        "                    if 'job_info_experience_num_months' in generated_data.columns:\n",
        "                        generated_data.loc[idx, 'job_info_experience_num_months'] = 12\n",
        "\n",
        "            elif pattern == 'experience_education_mismatch':\n",
        "                if all(col in generated_data.columns for col in ['job_info_education',\n",
        "                                                               'job_info_job_title',\n",
        "                                                               'job_info_experience_num_months']):\n",
        "                    generated_data.loc[idx, 'job_info_education'] = 'High School'\n",
        "                    generated_data.loc[idx, 'job_info_job_title'] = 'Senior Software Engineer'\n",
        "                    generated_data.loc[idx, 'job_info_experience_num_months'] = 6\n",
        "\n",
        "            elif pattern == 'application_timeline_mismatch':\n",
        "                if all(col in generated_data.columns for col in ['case_received_date',\n",
        "                                                               'decision_date']):\n",
        "                    # Create suspicious timeline patterns\n",
        "                    received_date = pd.Timestamp.now() - pd.Timedelta(days=np.random.randint(1, 30))\n",
        "                    decision_date = received_date + pd.Timedelta(days=np.random.randint(1, 5))\n",
        "\n",
        "                    generated_data.loc[idx, 'case_received_date'] = received_date\n",
        "                    generated_data.loc[idx, 'decision_date'] = decision_date\n",
        "\n",
        "        return generated_data\n",
        "\n",
        "    def train(self, epochs=100, batch_size=64):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, len(self.real_data), batch_size):\n",
        "                # Get real batch\n",
        "                real_batch = torch.FloatTensor(\n",
        "                    self.real_data.iloc[i:i+batch_size].values\n",
        "                ).to(self.device)\n",
        "                batch_size = len(real_batch)\n",
        "\n",
        "                # Generate fake data\n",
        "                noise = torch.randn(batch_size, self.input_dim).to(self.device)\n",
        "                fake_batch = self.generator(noise)\n",
        "\n",
        "                # Train Discriminator\n",
        "                self.d_optimizer.zero_grad()\n",
        "                real_output = self.discriminator(real_batch)\n",
        "                fake_output = self.discriminator(fake_batch.detach())\n",
        "\n",
        "                d_loss_real = self.criterion(\n",
        "                    real_output,\n",
        "                    torch.ones_like(real_output)\n",
        "                )\n",
        "                d_loss_fake = self.criterion(\n",
        "                    fake_output,\n",
        "                    torch.zeros_like(fake_output)\n",
        "                )\n",
        "                d_loss = (d_loss_real + d_loss_fake) / 2\n",
        "                d_loss.backward()\n",
        "                self.d_optimizer.step()\n",
        "\n",
        "                # Train Generator\n",
        "                self.g_optimizer.zero_grad()\n",
        "                fake_output = self.discriminator(fake_batch)\n",
        "                g_loss = self.criterion(\n",
        "                    fake_output,\n",
        "                    torch.ones_like(fake_output)\n",
        "                )\n",
        "                g_loss.backward()\n",
        "                self.g_optimizer.step()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}')\n",
        "\n",
        "    def generate_applications(self, num_samples=1000, include_fraud=True):\n",
        "        \"\"\"Generate synthetic applications\"\"\"\n",
        "        self.generator.eval()\n",
        "        with torch.no_grad():\n",
        "            noise = torch.randn(num_samples, self.input_dim).to(self.device)\n",
        "            generated_data = self.generator(noise).cpu().numpy()\n",
        "\n",
        "        # Convert to DataFrame with original column names\n",
        "        df_generated = pd.DataFrame(\n",
        "            generated_data,\n",
        "            columns=self.real_data.columns\n",
        "        )\n",
        "\n",
        "        if include_fraud:\n",
        "            df_generated = self.inject_fraud_patterns(df_generated)\n",
        "\n",
        "        return df_generated"
      ],
      "metadata": {
        "id": "DSmIqGMIO8pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "\"\"\"\n",
        "# Prepare data\n",
        "real_data = pd.DataFrame(...)  # Your real H1B application data\n",
        "\n",
        "# Initialize and train GAN\n",
        "gan = H1BGAN(real_data)\n",
        "gan.train(epochs=200, batch_size=64)\n",
        "\n",
        "# Generate synthetic applications with fraud patterns\n",
        "synthetic_applications = gan.generate_applications(num_samples=1000, include_fraud=True)\n",
        "\"\"\"\n",
        "gan = H1BGAN(df)"
      ],
      "metadata": {
        "id": "cYaNDWb-UAGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan.train(epochs=2, batch_size=256)"
      ],
      "metadata": {
        "id": "ZKpfJEvSPSQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_applications = gan.generate_applications(num_samples=1000, include_fraud=True)"
      ],
      "metadata": {
        "id": "oKwT3-3j0hv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['is_fraud'] = False"
      ],
      "metadata": {
        "id": "nvEqJQGIeLfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#traditional ML:\n",
        "#need another passthrough the model\n",
        "#fitting the model to the fraud/not fraud column - y values, save it to pickle\n",
        "#using ollama to get an endpoint running\n",
        "#langchain documentation to use that ollama to run llm assistant to provide suggestions on how we're prompting\n",
        "\n"
      ],
      "metadata": {
        "id": "Fo2NlJaHMlJq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}